{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Step 0: Import packages\n",
    "# ----------------------- \n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For displaying all of the columns in dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# For data modeling\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "# Use pip install xgboost if calls to xgboost return module not found error\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For metrics and helpful functions\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# For saving models\n",
    "import pickle\n",
    "\n",
    "# for displaying and modifying the working directory\n",
    "import os as os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\OneDrive\\\\Documents\\\\Advanced Capstone\\\\CSV'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Step 1: Load dataset into dataframe and verify\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Tell python where to find the dataset and load it to dataframe df0\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAdvanced Capstone\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCSV\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# absolute path, using \\ and r prefix\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df0 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR_comma_sep.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display the first 10 rows of the dataframe\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\OneDrive\\\\Documents\\\\Advanced Capstone\\\\CSV'"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 1: Load dataset into dataframe and verify\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Tell python where to find the dataset and load it to dataframe df0\n",
    "os.chdir(r'D:\\OneDrive\\Documents\\Advanced Capstone\\CSV') # absolute path, using \\ and r prefix\n",
    "df0 = pd.read_csv(\"HR_comma_sep.csv\")\n",
    "\n",
    "# Display the first 10 rows of the dataframe\n",
    "df0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Step 2: Initial Exploratory Data Analysis (EDA) and Cleaning\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Display basic information about the data \n",
    "df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics about the data \n",
    "df0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names \n",
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns that are misspelled, not easy to work with or that do not follow standard naming conventions\n",
    "df0 = df0.rename(columns={'Work_accident': 'work_accident',\n",
    "                          'average_montly_hours': 'average_monthly_hours',\n",
    "                          'time_spend_company': 'tenure',\n",
    "                          'Department': 'department'})\n",
    "\n",
    "# Display updated column names\n",
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df0.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df0.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a few of the duplicated rows\n",
    "df0[df0.duplicated()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and save results to dataframe to df1\n",
    "df1 = df0.drop_duplicates(keep='first')\n",
    "\n",
    "# Display first 10 rows of dataframe df1\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of the distribution of `tenure` to check for outliers\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title('Boxplot to Detect Outliers for Tenure', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "sns.boxplot(x=df1['tenure'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows containing outlier values for tenure\n",
    "\n",
    "# Calculate the 25th percentile\n",
    "percentile25 = df1['tenure'].quantile(0.25)\n",
    "\n",
    "# Calculate the 75th percentile\n",
    "percentile75 = df1['tenure'].quantile(0.75)\n",
    "\n",
    "# Calculate the interquartile range\n",
    "iqr = percentile75 - percentile25\n",
    "\n",
    "# Define the upper limit and lower limit for non-outlier values\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"Lower limit:\", lower_limit)\n",
    "print(\"Upper limit:\", upper_limit)\n",
    "\n",
    "# Identify subset of data containing outlier values for `tenure`\n",
    "outliers = df1[(df1['tenure'] > upper_limit) | (df1['tenure'] < lower_limit)]\n",
    "\n",
    "# Count the number of rows with outliers\n",
    "print(\"Number of rows with outliers for `tenure`:\", len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Step 3: Further Data Exploration\n",
    "# --------------------------------\n",
    "\n",
    "# Display number of people who stayed (0) and left (1)\n",
    "print(df1['left'].value_counts())\n",
    "print()\n",
    "\n",
    "# Display percentage of people who stayed (0) and left (1)\n",
    "print(df1['left'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots to help illustrate\n",
    "\n",
    "# Set figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
    "\n",
    "# Create boxplot showing `average_monthly_hours` distributions for `number_project`, comparing employees who stayed versus those who left\n",
    "sns.boxplot(data=df1, x='average_monthly_hours', y='number_project', hue='left', orient=\"h\", ax=ax[0])\n",
    "ax[0].invert_yaxis()\n",
    "ax[0].set_title('Monthly hours by Number of Projects', fontsize='14')\n",
    "\n",
    "# Create histogram showing distribution of `number_project`, comparing employees who stayed versus those who left\n",
    "tenure_stay = df1[df1['left']==0]['number_project']\n",
    "tenure_left = df1[df1['left']==1]['number_project']\n",
    "sns.histplot(data=df1, x='number_project', hue='left', multiple='dodge', shrink=2, ax=ax[1])\n",
    "ax[1].set_title('Number of Projects Histogram', fontsize='14')\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of people who worked 7 projects stayed (0) and left (1)\n",
    "df1[df1['number_project']==7]['left'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a scatterplot of `average_monthly_hours` versus `satisfaction_level`, comparing employees who stayed vs employees who left\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(data=df1, x='average_monthly_hours', y='satisfaction_level', hue='left', alpha=0.4)\n",
    "plt.axvline(x=166.67, color='#ff6361', label='166.67 hrs./mo.', ls='--')\n",
    "plt.legend(labels=['166.67 hrs./mo.', 'Stayed', 'Left'])\n",
    "plt.title('Monthly Hours by Satisfaction Level', fontsize='14');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More plots, this time looking at satisfaction level and tenure\n",
    "\n",
    "# Set figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
    "\n",
    "# Display boxplot showing distributions of `satisfaction_level` by tenure, comparing employees who stayed versus those who left\n",
    "sns.boxplot(data=df1, x='satisfaction_level', y='tenure', hue='left', orient=\"h\", ax=ax[0])\n",
    "ax[0].invert_yaxis()\n",
    "ax[0].set_title('Satisfaction by Tenure', fontsize='14')\n",
    "\n",
    "# Display histogram showing distribution of `tenure`, comparing employees who stayed versus those who left\n",
    "tenure_stay = df1[df1['left']==0]['tenure']\n",
    "tenure_left = df1[df1['left']==1]['tenure']\n",
    "sns.histplot(data=df1, x='tenure', hue='left', multiple='dodge', shrink=5, ax=ax[1])\n",
    "ax[1].set_title('Tenure Histogram', fontsize='14')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and median satisfaction scores for employees who left and employees who stayed\n",
    "# 0 = stayed, 1 = left\n",
    "df1.groupby(['left'])['satisfaction_level'].agg([np.mean,np.median])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of tenure vs salary\n",
    "\n",
    "# Set figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
    "\n",
    "# Define short-tenured employees\n",
    "tenure_short = df1[df1['tenure'] < 7]\n",
    "\n",
    "# Define long-tenured employees\n",
    "tenure_long = df1[df1['tenure'] > 6]\n",
    "\n",
    "# Plot short-tenured histogram\n",
    "sns.histplot(data=tenure_short, x='tenure', hue='salary', discrete=1, \n",
    "             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.5, ax=ax[0])\n",
    "ax[0].set_title('Salary Histogram by Tenure: 6 Years or Less', fontsize='14')\n",
    "\n",
    "# Plot long-tenured histogram\n",
    "sns.histplot(data=tenure_long, x='tenure', hue='salary', discrete=1, \n",
    "             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.4, ax=ax[1])\n",
    "ax[1].set_title('Salary Histogram by Tenure: 7 Years or More', fontsize='14');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a scatterplot of `average_monthly_hours` versus `last_evaluation`\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(data=df1, x='average_monthly_hours', y='last_evaluation', hue='left', alpha=0.4)\n",
    "plt.axvline(x=166.67, color='#ff6361', label='166.67 hrs./mo.', ls='--')\n",
    "plt.legend(labels=['166.67 hrs./mo.', 'Stayed', 'Left'])\n",
    "plt.title('Monthly Hours by Last Evaluation Score', fontsize='14');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plot of the relationship between `average_monthly_hours` and `promotion_last_5years`\n",
    "plt.figure(figsize=(16, 3))\n",
    "sns.scatterplot(data=df1, x='average_monthly_hours', y='promotion_last_5years', hue='left', alpha=0.4)\n",
    "plt.axvline(x=166.67, color='#ff6361', ls='--')\n",
    "plt.legend(labels=['166.67 hrs./mo.', 'Stayed', 'Left'])\n",
    "plt.title('Monthly Hours by Promotion in Last 5 Years', fontsize='14');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display counts of employees by department\n",
    "df1[\"department\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a histogram to show the distribution of employees who left and those who didn't by department\n",
    "plt.figure(figsize=(11,8))\n",
    "sns.histplot(data=df1, x='department', hue='left', discrete=1, \n",
    "             hue_order=[0, 1], multiple='dodge', shrink=.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Counts of Stayed and Left by Department', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a heatmap of the correlation\n",
    "corr = df1.corr(numeric_only = 1)\n",
    "plt.figure(figsize=(16, 9))\n",
    "heatmap = sns.heatmap(corr, vmin=None, vmax=None, annot=True, cmap=sns.color_palette(\"vlag\", as_cmap=True))\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':14}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 4: Build and Test a Logistic Regression Model\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Copy the dataframe\n",
    "df_enc = df1.copy()\n",
    "\n",
    "# Encode the `salary` column as an ordinal numeric category\n",
    "df_enc['salary'] = (\n",
    "    df_enc['salary'].astype('category')\n",
    "    .cat.set_categories(['low', 'medium', 'high'])\n",
    "    .cat.codes\n",
    ")\n",
    "\n",
    "# Dummy encode the `department` column\n",
    "df_enc = pd.get_dummies(df_enc, drop_first=False)\n",
    "\n",
    "# Display the new dataframe\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a heatmap to visualize how correlated variables are\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_enc[['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure']]\n",
    "            .corr(), annot=True, cmap=\"crest\")\n",
    "plt.title('Heatmap of the Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a stacked bar chart of number of employees by department, comparing those who left with those who stayed\n",
    "# 0 = stayed, 1 = left\n",
    "pd.crosstab(df1['department'], df1['left']).plot(kind ='bar',stacked=True)\n",
    "plt.title('Counts of Employees Who Left Vs. Stayed by Department')\n",
    "plt.ylabel('Employee count')\n",
    "plt.xlabel('Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows without outliers in `tenure` and save resulting dataframe in a new variable\n",
    "# Logistic regression is sensitive to outliers, so best not to use them\n",
    "df_logreg = df_enc[(df_enc['tenure'] >= lower_limit) & (df_enc['tenure'] <= upper_limit)]\n",
    "\n",
    "# Display first few rows of new dataframe\n",
    "df_logreg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the outcome (variable) to predict (y)\n",
    "y = df_logreg['left']\n",
    "\n",
    "# Display first few rows of y\n",
    "y.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features (X)\n",
    "X = df_logreg.drop('left', axis=1)\n",
    "\n",
    "# Display the first few rows of X \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build logistic regression model and fit it to the training data\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=500).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the logistic regression model to get predictions from the test data\n",
    "y_pred = log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "\n",
    "# Calculate values for confusion matrix\n",
    "log_cm = confusion_matrix(y_test, y_pred, labels=log_clf.classes_)\n",
    "\n",
    "# Create display of confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, \n",
    "                                  display_labels=log_clf.classes_)\n",
    "\n",
    "# Plot confusion matrix\n",
    "log_disp.plot(values_format='')\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are the predictions balanced?\n",
    "df_logreg['left'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report for logistic regression model\n",
    "target_names = ['Predicted would not leave', 'Predicted would leave']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Step 5: Build and Test a Tree-Based Model (Decision Tree and Random Forest)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Select the outcome (variable) to predict (y)\n",
    "y = df_enc['left']\n",
    "\n",
    "# Display the first few rows of y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features (X)\n",
    "X = df_enc.drop('left', axis=1)\n",
    "\n",
    "# Display the first few rows of X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision tree - Round 1\n",
    "\n",
    "# Instantiate model\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Assign a dictionary of hyperparameters to search over\n",
    "cv_params = {'max_depth':[4, 6, 8, None],\n",
    "             'min_samples_leaf': [2, 5, 1],\n",
    "             'min_samples_split': [2, 4, 6]\n",
    "             }\n",
    "\n",
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "tree1 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "\n",
    "tree1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best parameters\n",
    "tree1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best area under the curve (AUC) score on CV\n",
    "tree1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will provide all the scores from the grid search\n",
    "\n",
    "def make_results(model_name:str, model_object, metric:str):\n",
    "    '''\n",
    "    Arguments:\n",
    "        model_name (string): what you want the model to be called in the output table\n",
    "        model_object: a fit GridSearchCV object\n",
    "        metric (string): precision, recall, f1, accuracy, or auc\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores\n",
    "    for the model with the best mean 'metric' score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
    "    metric_dict = {'auc': 'mean_test_roc_auc',\n",
    "                   'precision': 'mean_test_precision',\n",
    "                   'recall': 'mean_test_recall',\n",
    "                   'f1': 'mean_test_f1',\n",
    "                   'accuracy': 'mean_test_accuracy'\n",
    "                  }\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the dataframe with the max score\n",
    "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
    "\n",
    "    # Extract Accuracy, precision, recall, and f1 score from that row\n",
    "    auc = best_estimator_results.mean_test_roc_auc\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision],\n",
    "                          'recall': [recall],\n",
    "                          'F1': [f1],\n",
    "                          'accuracy': [accuracy],\n",
    "                          'auc': [auc]\n",
    "                        })\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the scores\n",
    "tree1_cv_results = make_results('decision tree cv', tree1, 'auc')\n",
    "tree1_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a random forest model\n",
    "\n",
    "# Instantiate model\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Assign a dictionary of hyperparameters to search over\n",
    "cv_params = {'max_depth': [3,5, None], \n",
    "             'max_features': [1.0],\n",
    "             'max_samples': [0.7, 1.0],\n",
    "             'min_samples_leaf': [1,2,3],\n",
    "             'min_samples_split': [2,3,4],\n",
    "             'n_estimators': [300, 500],\n",
    "             }  \n",
    "\n",
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "rf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training data\n",
    "\n",
    "rf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where to save the model\n",
    "path = 'D:\\OneDrive\\Documents\\Advanced Capstone\\Pickle\\Pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a function to save the model\n",
    "\n",
    "def write_pickle(path, model_object, save_as:str):\n",
    "    '''\n",
    "    In: \n",
    "        path:         path of folder where you want to save the pickle\n",
    "        model_object: a model you want to pickle\n",
    "        save_as:      filename for how you want to save the model\n",
    "\n",
    "    Out: A call to pickle the model in the folder indicated\n",
    "    '''    \n",
    "\n",
    "    with open(path + save_as + '.pickle', 'wb') as to_write:\n",
    "        pickle.dump(model_object, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a function to load the model\n",
    "\n",
    "def read_pickle(path, saved_model_name:str):\n",
    "    '''\n",
    "    In: \n",
    "        path:             path to folder where you want to read from\n",
    "        saved_model_name: filename of pickled model you want to read in\n",
    "\n",
    "    Out: \n",
    "        model: the pickled model \n",
    "    '''\n",
    "    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n",
    "        model = pickle.load(to_read)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (write pickle)\n",
    "write_pickle(path, rf1, 'hr_rf1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (read pickle)\n",
    "rf1 = read_pickle(path, 'hr_rf1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best area under the curve (AUC) score\n",
    "rf1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best parameters\n",
    "rf1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all CV scores\n",
    "rf1_cv_results = make_results('random forest cv', rf1, 'auc')\n",
    "print(tree1_cv_results)\n",
    "print(rf1_cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will get the scores for the model's predictions\n",
    "\n",
    "def get_scores(model_name:str, model, X_test_data, y_test_data):\n",
    "    '''\n",
    "    Generate a table of test scores.\n",
    "\n",
    "    In: \n",
    "        model_name (string):  How you want your model to be named in the output table\n",
    "        model:                A fit GridSearchCV object\n",
    "        X_test_data:          numpy array of X_test data\n",
    "        y_test_data:          numpy array of y_test data\n",
    "\n",
    "    Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model\n",
    "    '''\n",
    "\n",
    "    preds = model.best_estimator_.predict(X_test_data)\n",
    "\n",
    "    auc = roc_auc_score(y_test_data, preds)\n",
    "    accuracy = accuracy_score(y_test_data, preds)\n",
    "    precision = precision_score(y_test_data, preds)\n",
    "    recall = recall_score(y_test_data, preds)\n",
    "    f1 = f1_score(y_test_data, preds)\n",
    "\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision], \n",
    "                          'recall': [recall],\n",
    "                          'f1': [f1],\n",
    "                          'accuracy': [accuracy],\n",
    "                          'AUC': [auc]\n",
    "                         })\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the test data to make predictions\n",
    "\n",
    "rf1_test_scores = get_scores('random forest1 test', rf1, X_test, y_test)\n",
    "rf1_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Step 6: Feature Engineering to Validate Random Forest Performance\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Drop `satisfaction_level` and save resulting dataframe in new variable\n",
    "df2 = df_enc.drop('satisfaction_level', axis=1)\n",
    "\n",
    "# Display first 10 rows of new dataframe\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `overworked` column. For the moment, it's the same as average monthly hours.\n",
    "df2['overworked'] = df2['average_monthly_hours']\n",
    "\n",
    "# Display min and max values for overworked\n",
    "print('Min hours:', df2['overworked'].min())\n",
    "print('Max hours:', df2['overworked'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define `overworked` as working more than 175 hrs per month (avg is about 166.67)\n",
    "df2['overworked'] = (df2['overworked'] > 175).astype(int)\n",
    "\n",
    "# Display first few rows of new column\n",
    "df2['overworked'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the `average_monthly_hours` column\n",
    "df2 = df2.drop('average_monthly_hours', axis=1)\n",
    "\n",
    "# Display first few rows of resulting dataframe\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the outcome (variable) to predict (y)\n",
    "y = df2['left']\n",
    "\n",
    "# Select the features (X)\n",
    "X = df2.drop('left', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat decision tree - Round 2\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Assign a dictionary of hyperparameters to search over\n",
    "cv_params = {'max_depth':[4, 6, 8, None],\n",
    "             'min_samples_leaf': [2, 5, 1],\n",
    "             'min_samples_split': [2, 4, 6]\n",
    "             }\n",
    "\n",
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "tree2 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "tree2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best parameters\n",
    "tree2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displ ay the best AUC score on CV\n",
    "tree2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all CV scores\n",
    "tree2_cv_results = make_results('decision tree2 cv', tree2, 'auc')\n",
    "print(tree1_cv_results)\n",
    "print(tree2_cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up random forest model\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Assign a dictionary of hyperparameters to search over\n",
    "cv_params = {'max_depth': [3,5, None], \n",
    "             'max_features': [1.0],\n",
    "             'max_samples': [0.7, 1.0],\n",
    "             'min_samples_leaf': [1,2,3],\n",
    "             'min_samples_split': [2,3,4],\n",
    "             'n_estimators': [300, 500],\n",
    "             }  \n",
    "\n",
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "rf2 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "rf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model (write pickle)\n",
    "write_pickle(path, rf2, 'hr_rf2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (read pickle)\n",
    "rf2 = read_pickle(path, 'hr_rf2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters\n",
    "rf2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best AUC score on CV\n",
    "rf2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all CV scores\n",
    "rf2_cv_results = make_results('random forest2 cv', rf2, 'auc')\n",
    "print(tree2_cv_results)\n",
    "print(rf2_cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test data\n",
    "rf2_test_scores = get_scores('random forest2 test', rf2, X_test, y_test)\n",
    "rf2_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values for confusion matrix\n",
    "preds = rf2.best_estimator_.predict(X_test)\n",
    "cm = confusion_matrix(y_test, preds, labels=rf2.classes_)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=rf2.classes_)\n",
    "disp.plot(values_format='');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree\n",
    "plt.figure(figsize=(85,20))\n",
    "plot_tree(tree2.best_estimator_, max_depth=6, fontsize=14, feature_names=X.columns, \n",
    "          class_names={0:'stayed', 1:'left'}, filled=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the feature importance for the decision tree\n",
    "\n",
    "tree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, \n",
    "                                 columns=['gini_importance'], \n",
    "                                 index=X.columns\n",
    "                                )\n",
    "tree2_importances = tree2_importances.sort_values(by='gini_importance', ascending=False)\n",
    "\n",
    "# Only extract the features with importances > 0\n",
    "tree2_importances = tree2_importances[tree2_importances['gini_importance'] != 0]\n",
    "tree2_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "\n",
    "sns.barplot(data=tree2_importances, x=\"gini_importance\", y=tree2_importances.index, orient='h', color='Blue')\n",
    "plt.title(\"Decision Tree: Feature Importance in Predicting Employees Leaving\", fontsize=11)\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random forest feature importance\n",
    "\n",
    "# Get feature importances\n",
    "feat_impt = rf2.best_estimator_.feature_importances_\n",
    "\n",
    "# Get indices of top 10 features\n",
    "ind = np.argpartition(rf2.best_estimator_.feature_importances_, -10)[-10:]\n",
    "\n",
    "# Get column labels of top 10 features \n",
    "feat = X.columns[ind]\n",
    "\n",
    "# Filter `feat_impt` to consist of top 10 feature importances\n",
    "feat_impt = feat_impt[ind]\n",
    "\n",
    "y_df = pd.DataFrame({\"Feature\":feat,\"Importance\":feat_impt})\n",
    "y_sort_df = y_df.sort_values(\"Importance\")\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "y_sort_df.plot(kind='barh',ax=ax1,x=\"Feature\",y=\"Importance\", color='orange')\n",
    "\n",
    "ax1.set_title(\"Random Forest: Feature Importance in Predicting Employees Leaving\", fontsize=11)\n",
    "ax1.set_ylabel(\"Feature\")\n",
    "ax1.set_xlabel(\"Importance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
